{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdb490d-d096-4528-a67f-485af957eb18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements_dev.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6399d8b3-a86d-463a-b66c-1adf64bfe7a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget \"https://cloud.tsinghua.edu.cn/f/498512c3c1724558830d/?dl=1\" -O parquet-train.arrow\n",
    "!mkdir -p ./model\n",
    "!wget \"https://cloud.tsinghua.edu.cn/f/8bfd19e6cb1a4a289c1b/?dl=1\" -O added_tokens.json\n",
    "!wget \"https://cloud.tsinghua.edu.cn/f/231ddebf6caf49b38ce8/?dl=1\" -O config.json\n",
    "!wget \"https://cloud.tsinghua.edu.cn/f/79e402dcc503430db9a1/?dl=1\" -O merges.txt\n",
    "!wget \"https://cloud.tsinghua.edu.cn/f/001e6641d7324635bc77/?dl=1\" -O special_tokens_map.json\n",
    "!wget \"https://cloud.tsinghua.edu.cn/f/2d68e62358da4b7f94e6/?dl=1\" -O tokenizer.json\n",
    "!wget \"https://cloud.tsinghua.edu.cn/f/5ebcd4f2380147e3bee8/?dl=1\" -O tokenizer_config.json\n",
    "!wget \"https://cloud.tsinghua.edu.cn/f/34aa75355590497ba28b/?dl=1\" -O vocab.json\n",
    "!wget \"https://cloud.tsinghua.edu.cn/f/cd59c04366674ab592b0/?dl=1\" -O pytorch_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b0edcd4-5905-4fce-85d2-bee88f1297de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "02f96f3f-67df-480d-83bf-b46f0e79e32d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abb8e06-658e-43cd-919d-518896679cfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/zphang/transformers.git@neox20b\n",
    "!pip install tensorboard==1.15.0\n",
    "!pip install bitsandbytes-cuda117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7b022d-29bb-447d-b18d-e94d192d7eaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!conda install cudatoolkit\n",
    "!pip install bitsandbytes\n",
    "!pip install --upgrade accelerate\n",
    "!pip install --quiet git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d70115f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PATH=/usr/local/cuda-11.7/bin${PATH:+:${PATH}}\n",
    "!export LD_LIBRARY_PATH=/usr/local/cuda-11.7/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5a27dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/timdettmers/bitsandbytes.git\n",
    "!python bitsandbytes/setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f903c49d-8aef-41bd-8e52-4ae0b8410d9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "timestamp = '2023-05-15 04:04:55'\n",
    "model_name = 'dolly'\n",
    "input_model = 'EleutherAI/pythia-2.8b'\n",
    "checkpoint_dir_name = f\"{model_name}_{timestamp}\"\n",
    "deepspeed_config = f'{os.getcwd()}/config/ds_z3_bf16_config.json'\n",
    "local_output_dir = f\"./{checkpoint_dir_name}\"\n",
    "dbfs_output_dir = ''\n",
    "tensorboard_display_dir = f\"{local_output_dir}/runs\"\n",
    "DATASET_FILE_PATH = f'{os.getcwd()}/parquet-train.arrow'\n",
    "MODEL_PATH = f'{os.getcwd()}/model/'\n",
    "CUDA_VISIBLE_DEVICES = 1\n",
    "PATH = f\"/usr/local/cuda/bin:{os.getenv('PATH')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c13f517b-62e0-4ea0-a178-0a15d830bed5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-15 12:09:10,032] [WARNING] [runner.py:186:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2023-05-15 12:09:10,046] [INFO] [runner.py:550:main] cmd = /home/ec2-user/anaconda3/envs/pytorch_p39/bin/python3.9 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --module --enable_each_rank_log=None training.trainer --deepspeed config/ds_z3_bf16_config.json --epochs 1 --local-output-dir 2023-05-15 04:04:55 --dbfs-output-dir  --per-device-train-batch-size 1 --per-device-eval-batch-size 1 --lr 1e-5 --warmup-steps 50 --input-model EleutherAI/pythia-2.8b --logging-steps 1000 --test-size 10\n",
      "[2023-05-15 12:09:12,155] [INFO] [launch.py:142:main] WORLD INFO DICT: {'localhost': [0]}\n",
      "[2023-05-15 12:09:12,155] [INFO] [launch.py:148:main] nnodes=1, num_local_procs=1, node_rank=0\n",
      "[2023-05-15 12:09:12,155] [INFO] [launch.py:161:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
      "[2023-05-15 12:09:12,155] [INFO] [launch.py:162:main] dist_world_size=1\n",
      "[2023-05-15 12:09:12,155] [INFO] [launch.py:164:main] Setting CUDA_VISIBLE_DEVICES=0\n",
      "2023-05-15 12:09:17 INFO [__main__] Loading tokenizer for EleutherAI/pythia-2.8b\n",
      "2023-05-15 12:09:17 INFO [__main__] Loading model for EleutherAI/pythia-2.8b\n",
      "2023-05-15 12:09:47 INFO [__main__] Found max lenth: 2048\n",
      "2023-05-15 12:09:47 INFO [__main__] Loading dataset from databricks/databricks-dolly-15k\n",
      "2023-05-15 12:09:50 WARNING [datasets.builder] Found cached dataset json (/home/ec2-user/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-6e0f9ea7eaa0ee08/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 576.06it/s]\n",
      "2023-05-15 12:09:50 INFO [__main__] Found 15011 rows\n",
      "2023-05-15 12:09:50 WARNING [datasets.arrow_dataset] Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-6e0f9ea7eaa0ee08/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-1215174d9a18fb14.arrow\n",
      "2023-05-15 12:09:50 INFO [__main__] Preprocessing dataset\n",
      "2023-05-15 12:09:50 WARNING [datasets.arrow_dataset] Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-6e0f9ea7eaa0ee08/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-666b95978f858a12.arrow\n",
      "2023-05-15 12:09:50 INFO [__main__] Processed dataset has 15011 rows\n",
      "2023-05-15 12:09:50 WARNING [datasets.arrow_dataset] Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-6e0f9ea7eaa0ee08/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-61e704973abea014.arrow\n",
      "2023-05-15 12:09:50 INFO [__main__] Processed dataset has 14977 rows after filtering for truncated records\n",
      "2023-05-15 12:09:50 INFO [__main__] Shuffling dataset\n",
      "2023-05-15 12:09:50 WARNING [datasets.arrow_dataset] Loading cached shuffled indices for dataset at /home/ec2-user/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-6e0f9ea7eaa0ee08/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-7ca227af1001a239.arrow\n",
      "2023-05-15 12:09:50 INFO [__main__] Done preprocessing\n",
      "2023-05-15 12:09:50 WARNING [datasets.arrow_dataset] Loading cached split indices for dataset at /home/ec2-user/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-6e0f9ea7eaa0ee08/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-2a81d52b54833e22.arrow and /home/ec2-user/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-6e0f9ea7eaa0ee08/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-e02eb44a41c1e38f.arrow\n",
      "2023-05-15 12:09:50 INFO [__main__] Train data size: 14967\n",
      "2023-05-15 12:09:50 INFO [__main__] Test data size: 10\n",
      "/home/ec2-user/SageMaker/dolly/training/trainer.py:236: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "  logger.warn(\"Will NOT save to DBFS\")\n",
      "2023-05-15 12:09:50 WARNING [__main__] Will NOT save to DBFS\n",
      "[2023-05-15 12:09:50,077] [INFO] [comm.py:652:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "2023-05-15 12:09:50 INFO [torch.distributed.distributed_c10d] Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "2023-05-15 12:09:50 INFO [torch.distributed.distributed_c10d] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.\n",
      "2023-05-15 12:09:50 INFO [__main__] Instantiating Trainer\n",
      "2023-05-15 12:09:50 INFO [__main__] Training\n",
      "2023-05-15 12:09:53 INFO [torch.distributed.distributed_c10d] Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "2023-05-15 12:09:53 INFO [torch.distributed.distributed_c10d] Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 1 nodes.\n",
      "Using /home/ec2-user/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ec2-user/.cache/torch_extensions/py39_cu117/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.19611859321594238 seconds\n",
      "Using /home/ec2-user/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/ec2-user/.cache/torch_extensions/py39_cu117/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.1953907012939453 seconds\n",
      "Parameter Offload: Total persistent parameters: 1070080 in 258 params\n",
      "2023-05-15 12:10:06 ERROR [__main__] main failed\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/SageMaker/dolly/training/trainer.py\", line 329, in <module>\n",
      "    main()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/click/core.py\", line 1130, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/click/core.py\", line 1055, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/click/core.py\", line 1404, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/click/core.py\", line 760, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"/home/ec2-user/SageMaker/dolly/training/trainer.py\", line 321, in main\n",
      "    train(**kwargs)\n",
      "  File \"/home/ec2-user/SageMaker/dolly/training/trainer.py\", line 276, in train\n",
      "    trainer.train()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/transformers/trainer.py\", line 1664, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/transformers/trainer.py\", line 1741, in _inner_training_loop\n",
      "    deepspeed_engine, optimizer, lr_scheduler = deepspeed_init(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/transformers/deepspeed.py\", line 378, in deepspeed_init\n",
      "    deepspeed_engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/deepspeed/__init__.py\", line 125, in initialize\n",
      "    engine = DeepSpeedEngine(args=args,\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/deepspeed/runtime/engine.py\", line 340, in __init__\n",
      "    self._configure_optimizer(optimizer, model_parameters)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/deepspeed/runtime/engine.py\", line 1298, in _configure_optimizer\n",
      "    self.optimizer = self._configure_zero_optimizer(basic_optimizer)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/deepspeed/runtime/engine.py\", line 1599, in _configure_zero_optimizer\n",
      "    optimizer = DeepSpeedZeroOptimizer_Stage3(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/deepspeed/runtime/zero/stage3.py\", line 312, in __init__\n",
      "    self._setup_for_real_optimizer()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/deepspeed/runtime/zero/stage3.py\", line 362, in _setup_for_real_optimizer\n",
      "    self._create_fp32_partitions()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/deepspeed/runtime/zero/stage3.py\", line 780, in _create_fp32_partitions\n",
      "    self.fp16_partitioned_groups_flat[i].to(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.78 GiB (GPU 0; 14.62 GiB total capacity; 10.46 GiB already allocated; 3.67 GiB free; 10.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ec2-user/SageMaker/dolly/training/trainer.py\", line 329, in <module>\n",
      "    main()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/click/core.py\", line 1130, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/click/core.py\", line 1055, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/click/core.py\", line 1404, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/click/core.py\", line 760, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"/home/ec2-user/SageMaker/dolly/training/trainer.py\", line 321, in main\n",
      "    train(**kwargs)\n",
      "  File \"/home/ec2-user/SageMaker/dolly/training/trainer.py\", line 276, in train\n",
      "    trainer.train()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/transformers/trainer.py\", line 1664, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/transformers/trainer.py\", line 1741, in _inner_training_loop\n",
      "    deepspeed_engine, optimizer, lr_scheduler = deepspeed_init(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/transformers/deepspeed.py\", line 378, in deepspeed_init\n",
      "    deepspeed_engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/deepspeed/__init__.py\", line 125, in initialize\n",
      "    engine = DeepSpeedEngine(args=args,\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/deepspeed/runtime/engine.py\", line 340, in __init__\n",
      "    self._configure_optimizer(optimizer, model_parameters)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/deepspeed/runtime/engine.py\", line 1298, in _configure_optimizer\n",
      "    self.optimizer = self._configure_zero_optimizer(basic_optimizer)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/deepspeed/runtime/engine.py\", line 1599, in _configure_zero_optimizer\n",
      "    optimizer = DeepSpeedZeroOptimizer_Stage3(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/deepspeed/runtime/zero/stage3.py\", line 312, in __init__\n",
      "    self._setup_for_real_optimizer()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/deepspeed/runtime/zero/stage3.py\", line 362, in _setup_for_real_optimizer\n",
      "    self._create_fp32_partitions()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/deepspeed/runtime/zero/stage3.py\", line 780, in _create_fp32_partitions\n",
      "    self.fp16_partitioned_groups_flat[i].to(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.78 GiB (GPU 0; 14.62 GiB total capacity; 10.46 GiB already allocated; 3.67 GiB free; 10.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "[2023-05-15 12:10:08,216] [INFO] [launch.py:318:sigkill_handler] Killing subprocess 29310\n",
      "[2023-05-15 12:10:08,217] [ERROR] [launch.py:324:sigkill_handler] ['/home/ec2-user/anaconda3/envs/pytorch_p39/bin/python3.9', '-u', '-m', 'training.trainer', '--local_rank=0', '--deepspeed', 'config/ds_z3_bf16_config.json', '--epochs', '1', '--local-output-dir', '2023-05-15 04:04:55', '--dbfs-output-dir', '', '--per-device-train-batch-size', '1', '--per-device-eval-batch-size', '1', '--lr', '1e-5', '--warmup-steps', '50', '--input-model', 'EleutherAI/pythia-2.8b', '--logging-steps', '1000', '--test-size', '10'] exits with return code = 1\n"
     ]
    }
   ],
   "source": [
    "!deepspeed --num_gpus=1 \\\n",
    "    --module training.trainer \\\n",
    "    --deepspeed \"config/ds_z3_bf16_config.json\" \\\n",
    "    --epochs 1 \\\n",
    "    --local-output-dir \"2023-05-15 04:04:55\" \\\n",
    "    --dbfs-output-dir \"\" \\\n",
    "    --per-device-train-batch-size 1 \\\n",
    "    --per-device-eval-batch-size 1 \\\n",
    "    --lr 1e-5 \\\n",
    "    --warmup-steps 50 \\\n",
    "    --input-model \"EleutherAI/pythia-2.8b\" \\\n",
    "    --logging-steps 1000 \\\n",
    "    --test-size 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c72f60-1164-4955-bf2d-584bd23a988f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f23b2da6-ba7a-414c-98f2-e1cc3905e7b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 15 12:03:37 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   35C    P8    14W /  70W |      2MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaa8ed8-3c16-4fcf-87d5-3c13fa42fd2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ba1899-450b-4633-9208-600ef5ce4bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576e12bf-3ded-4567-8539-b32b2dd69940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
